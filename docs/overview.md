# Architecture Overview

Ledger is InferaDB's storage layer—a blockchain database for cryptographically verifiable auditing. It exposes a gRPC API and uses Raft consensus for replication.

## Design Goals

1. **High Performance**: Sub-millisecond reads, <50ms writes
2. **Fault Isolation**: Chain-per-vault ensures failures don't cascade
3. **Cryptographic Auditability**: Every state provable against a merkle root
4. **Fearless Replication**: Raft consensus with self-verifying chain structure
5. **Operational Simplicity**: Snapshot recovery, zero-downtime migrations

## Terminology

| Term             | Definition                                                                                                                           |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **Namespace**    | Storage unit per organization. Contains entities, vaults, relationships. Isolated with separate Raft consensus per shard.            |
| **Vault**        | Relationship store within a namespace. Maintains its own cryptographic chain (separate `state_root`, `previous_hash`, block height). |
| **Entity**       | Key-value data in a namespace (users, teams, clients, sessions). Supports TTL, versioning, conditional writes.                       |
| **Relationship** | Authorization tuple: `(resource, relation, subject)`. Used by Engine for permission checks.                                          |
| **`_system`**    | Special namespace for global data: user accounts and namespace routing. Replicated to all nodes.                                     |

## Hierarchy

```
_system namespace (global)
├── Users (global accounts)
└── Namespace routing table

org namespace (per-organization)
├── Organization metadata
├── Members, Teams, Clients
├── Vault A (relationships + grants) ─── separate chain
├── Vault B (relationships + grants) ─── separate chain
└── ...
```

## Isolation Model

| Level     | Isolated                          | Shared                 |
| --------- | --------------------------------- | ---------------------- |
| Namespace | Entities, vaults, keys            | Shard (Raft consensus) |
| Vault     | Cryptographic chain, `state_root` | Namespace storage      |
| Shard     | Physical nodes                    | Cluster                |

Namespaces share physical nodes and Raft groups but maintain independent data. Vaults within a namespace share storage but maintain separate cryptographic chains.

## Shard Groups

A naive 1:1 namespace-to-Raft mapping doesn't scale. Multiple namespaces share a single Raft group (shard). Within each namespace, vaults maintain independent cryptographic chains.

**What's shared (per shard group):**

- Raft consensus (single leader election, heartbeat, log)
- Physical replication
- Snapshot coordination

**What's independent (per vault):**

- Cryptographic chain (height, previous_hash, state_root)
- State tree and indexes
- Merkle proofs and verification

### Scaling

| Namespaces | Vaults (avg 3/ns) | Vaults/Shard | Shard Groups | Raft Machines | Heartbeats |
| ---------- | ----------------- | ------------ | ------------ | ------------- | ---------- |
| 1,000      | 3,000             | 100          | 30           | 90            | ~600/sec   |
| 10,000     | 30,000            | 100          | 300          | 900           | ~6K/sec    |
| 100,000    | 300,000           | 1,000        | 300          | 900           | ~6K/sec    |

### Shard Assignment

- New namespaces assigned to shard with lowest load
- All vaults within a namespace reside on the same shard (locality)
- Namespaces can migrate between shards (coordinated via `_system`)
- High-traffic namespaces can be promoted to dedicated shards

## Block Structure

Blocks are packed into `ShardBlock` entries for Raft efficiency:

```rust
struct ShardBlock {
    shard_id: ShardId,
    shard_height: u64,
    previous_shard_hash: Hash,
    vault_entries: Vec<VaultEntry>,
    timestamp: DateTime<Utc>,
    leader_id: NodeId,
    term: u64,
    committed_index: u64,
}

struct VaultEntry {
    namespace_id: NamespaceId,
    vault_id: VaultId,
    vault_height: u64,
    previous_vault_hash: Hash,
    transactions: Vec<Transaction>,
    tx_merkle_root: Hash,
    state_root: Hash,
}
```

Each `VaultEntry` computes `tx_merkle_root` from its own transactions only. Clients verify transaction inclusion without accessing other namespaces' data.

**Physical storage**: ShardBlocks only (single write path). VaultBlocks extracted on-demand via O(1) offset lookup.

## ID Generation

Ledger uses **leader-assigned sequential IDs**. The leader assigns IDs during block construction; followers replay the same IDs from the Raft log.

| ID Type       | Size     | Generated By  | Strategy                                             |
| ------------- | -------- | ------------- | ---------------------------------------------------- |
| `NamespaceId` | int64    | Ledger Leader | Sequential from `_meta:seq:namespace` (0 = \_system) |
| `VaultId`     | int64    | Ledger Leader | Sequential from `_meta:seq:vault`                    |
| `UserId`      | int64    | Ledger Leader | Sequential from `_meta:seq:user`                     |
| `UserEmailId` | int64    | Ledger Leader | Sequential from `_meta:seq:user_email`               |
| `ClientId`    | string   | Control       | Opaque API key identifier                            |
| `TxId`        | 16 bytes | Ledger Leader | UUID assigned during block creation                  |
| `NodeId`      | string   | Admin         | Configuration (hostname or UUID)                     |
| `ShardId`     | uint32   | Admin         | Sequential at shard creation                         |

**Sequence counters** stored in `_system` namespace:

```
_meta:seq:namespace    → next NamespaceId (starts at 1; 0 = _system)
_meta:seq:vault        → next VaultId
_meta:seq:user         → next UserId
_meta:seq:user_email   → next UserEmailId
_meta:seq:email_verify → next TokenId
```

**Determinism guarantee:**

1. Leader allocates ID and increments counter atomically (same block)
2. Block proposed to Raft with the assigned ID
3. Followers apply the block—they read the ID from the log, never generate
4. All nodes arrive at identical state

## Key Patterns

### In `_system` (namespace_id = 0)

| Entity Type       | Key Pattern                  | Example                        |
| ----------------- | ---------------------------- | ------------------------------ |
| User              | `user:{id}`                  | `user:1`                       |
| User email        | `user_email:{id}`            | `user_email:1`                 |
| Email index       | `_idx:email:{email}`         | `_idx:email:alice@example.com` |
| User emails index | `_idx:user_emails:{user_id}` | `_idx:user_emails:1`           |
| Namespace routing | `ns:{namespace_id}`          | `ns:1`                         |
| Node info         | `node:{id}`                  | `node:A`                       |
| Sequence counter  | `_meta:seq:{entity_type}`    | `_meta:seq:user`               |

### In Organization Namespace

| Entity Type        | Key Pattern                           | Example                       |
| ------------------ | ------------------------------------- | ----------------------------- |
| Org metadata       | `_meta`                               | `_meta`                       |
| Member             | `member:{id}`                         | `member:100`                  |
| Member index       | `_idx:member:user:{user_id}`          | `_idx:member:user:1`          |
| Team               | `team:{id}`                           | `team:200`                    |
| Team members index | `_idx:team_members:{team_id}`         | `_idx:team_members:200`       |
| Client             | `client:{id}`                         | `client:400`                  |
| Session            | `session:{token}`                     | `session:abc123`              |
| Vault metadata     | `vault:{vault_id}:_meta`              | `vault:1:_meta`               |
| User grant         | `vault:{vault_id}:grant:user:{id}`    | `vault:1:grant:user:500`      |
| Team grant         | `vault:{vault_id}:grant:team:{id}`    | `vault:1:grant:team:501`      |
| Relationship       | `rel:{resource}#{relation}@{subject}` | `rel:doc:1#viewer@user:alice` |

### Index Patterns

| Relationship | Index Pattern                     | Value           | Example                              |
| ------------ | --------------------------------- | --------------- | ------------------------------------ |
| 1:1 lookup   | `_idx:{type}:{lookup_key}`        | entity_id       | `_idx:email:alice@example.com` → `1` |
| 1:many list  | `_idx:{parent_type}s:{parent_id}` | [child_id, ...] | `_idx:user_emails:1` → `[1, 2]`      |

**Atomicity requirement**: Entities and their indexes must be created/deleted atomically in the same transaction.

## Network Protocol

Ledger uses **gRPC/HTTP/2** for both client-facing APIs and inter-node Raft consensus.

| Factor                 | gRPC/HTTP/2                                  | Custom TCP Protocol                |
| ---------------------- | -------------------------------------------- | ---------------------------------- |
| Development velocity   | High (generated clients, rich tooling)       | Low                                |
| Operational complexity | Low (standard load balancers, observability) | High                               |
| Performance            | ~0.5ms per RPC overhead                      | ~0.1-0.2ms theoretical improvement |

At target scale (<50K ops/sec per cluster), the ~0.3ms overhead is negligible compared to consensus latency (~2-5ms) and disk I/O.

### Serialization

```
Client Request
    ↓
[1] Proto → Internal Types (gRPC decode)
    ↓
[2] Internal Types → Postcard (Raft log serialization)
    ↓
[3] Postcard → Disk (B-tree write)
```

**Why postcard for storage**: Proto optimizes for wire transmission; postcard optimizes for in-process speed (no schema overhead, minimal allocation).

## Performance Targets

### Same Datacenter (3-node cluster, 1M keys)

| Metric             | Target          | Rationale                                   |
| ------------------ | --------------- | ------------------------------------------- |
| Read (p50)         | <0.5ms          | B-tree lookup + gRPC                        |
| Read (p99)         | <2ms            | Tail latency from GC/compaction             |
| Read + proof (p99) | <10ms           | Bucket-based O(k) proof                     |
| Write (p50)        | <10ms           | Raft RTT + fsync                            |
| Write (p99)        | <50ms           | Includes state_root computation             |
| Write throughput   | 5,000 tx/sec    | 100 tx/batch × 50 batches/sec               |
| Read throughput    | 100,000 req/sec | Per node, follower reads scale horizontally |

### Multi-Region (~50ms RTT)

| Metric      | Target                            |
| ----------- | --------------------------------- |
| Read (p99)  | <5ms (follower reads still local) |
| Write (p99) | <150ms (dominated by network RTT) |

## Limitations

| Dimension        | Recommended Max | Hard Limit        | Bottleneck                    |
| ---------------- | --------------- | ----------------- | ----------------------------- |
| Vaults per shard | 1,000           | 10,000            | Shard block size, memory      |
| Shard groups     | 10,000          | ~100,000          | Cluster coordination overhead |
| Total vaults     | 10M             | ~100M             | `_system` registry size       |
| Keys per vault   | 10M             | ~100M             | State tree memory             |
| Transactions/sec | 5,000 per shard | ~50,000 per shard | Raft throughput               |

### Not Supported

- Cross-vault transactions (by design)
- Byzantine fault tolerance (trusted network assumption)
- Sub-millisecond writes (consensus overhead)
- Infinite retention (storage costs)
- Instant per-key merkle proofs (hybrid storage trade-off)
